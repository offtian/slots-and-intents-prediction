{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jzT1V_KU_NG"
   },
   "source": [
    "# Applied Science Take-Home Exercise\n",
    "This notebook can act as a boilerplate to help you along the road, do not feel obliged to submit everything via a notebook or make your customizations/ do things differently to how it is presented here. \n",
    "\n",
    "In `semantic_parsing_dialog` there are three functions that should help you: \n",
    "- `load_dataset()` which loads a local tsv file as a huggingface Dataset\n",
    "- `postprocess_test()` which helps clean up predictions so that \n",
    "- `evaluate_predictions()` can be used to produce various model evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### What we are looking for:\n",
    "- Can you create a basic baseline model? Why have you chosen this implementation? \n",
    "- What are the steps have you taken (please make sure your thinking is well commented) \n",
    "- Try to experiment with methods to improve your basic baseline architecture \n",
    "- How are you evaluating & comparing your training runs? \n",
    "- Given more time what would you do? \n",
    "    - architecture strategies\n",
    "    - relevant research papers\n",
    "\n",
    "### Submission\n",
    "- Check-in your solution to a new branch and create a PR (`!git checkout -b 'submission'`)\n",
    "- Please make sure to include your predictions using two files called **data/submission_mce.tsv** and **data/submission_ccf.tsv**. The former should contain the prediction of the model trained with cross-entropy loss and the latter file containing the results of training with a custom cost function written by you. Make sure to compare the result of these two experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQsiFVc9YXpx"
   },
   "source": [
    "# Set Up\n",
    "\n",
    "The first cells are just preparing the environment and mounting the required volumes in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "# sys.path.append('/content/gdrive/MyDrive/<Your_Folder>/semantic_parsing_dialog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RmLmEszac7C",
    "outputId": "b4fa7317-439e-491a-9e5a-5ebc0b278f36"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an experiment tracking tool of your own choice, here we use Wandb \n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check you are connected to a GPU-enabled instance.\n",
    "\n",
    "print(torch.cuda.is_available()) # should be TRUE if GPU is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_parsing_dialog import ROOT \n",
    "from semantic_parsing_dialog import device\n",
    "\n",
    "from semantic_parsing_dialog.utils import load_datasets, load_semantic_vocab\n",
    "from semantic_parsing_dialog.vocab import get_intents, get_slots\n",
    "\n",
    "\n",
    "datapath = os.path.join(ROOT, 'data')\n",
    "\n",
    "dataset = load_datasets(datapath)\n",
    "semantic_vocab = load_semantic_vocab(datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUA22yIjW2S3"
   },
   "source": [
    "# Task Data Analysis\n",
    "\n",
    "Before jumping into the exercise, we'll have a look at some simple statistics of the task-oriented semantical vocabulary\n",
    "\n",
    "#### Slots and intents\n",
    "\n",
    "A quick analysis to have a feeling of the variability in terms of complexity for the different intents.\n",
    "\n",
    "Here we look at co-occurrences of intents and certain slots within the same query - without caring about whether slots are part of its semantics or part of other intents that form the composed query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "df['intent'] = df['representation'].map(get_intents)\n",
    "df['slot'] = df['representation'].map(get_slots)\n",
    "\n",
    "df['n_intent'] = df['intent'].map(len)\n",
    "df['n_slot'] = df['slot'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6RcUsjtac7F"
   },
   "outputs": [],
   "source": [
    "# TODO: Inspect any other aspect of the dataset relevant to the task at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22GUj5ukW_mr"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn7U9sncac7F"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuurupMsac7F"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_name_or_path = '' # TODO: write name\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# TODO: \n",
    "max_source_length = \n",
    "max_target_length = \n",
    "\n",
    "model.config.max_length = max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def extend_vocabulary(tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM, tokens: List[str]) -> None:\n",
    "    \"\"\" Extends the model's original vocabulary to accommodate new tokens\n",
    "    \n",
    "    The are added at the end of the tokenizer's vocabulary\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxS0m6MXac7F"
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training with cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  ...\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "from semantic_parsing_dialog import postprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5mzwPz5ac7G"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample):\n",
    "    # TODO\n",
    "    #############\n",
    "    \n",
    "    \n",
    "    #############\n",
    "\n",
    "    # Replace all tokenizer.pad_token_id in the labels by -100\n",
    "    # when we want to ignore padding in the loss.\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-L_0LoDXEEr"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "tokenized_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o44BCAZTXGXy"
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "data_collator = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9HXv1U_XGph"
   },
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE\n",
    "# you may want to alter the hyperparams\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    learning_rate=5e-1,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=10,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=..,\n",
    "    # callbacks=..\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozvODvYFac7H"
   },
   "outputs": [],
   "source": [
    "# log experiments\n",
    "# depending on how you implement the model, logging may be done differently. \n",
    "with wandb.init(project=\"semantic_parsing_dialog\") as run:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training with a custom loss function\n",
    "\n",
    "There are several intents with long tails in the training dataset, i.e. co-occurrence with a large cardinality of slots with low volume/representation in the training set. These cases could present learning challenges for the model. Incorporate a custom loss function in the training pipeline that takes into account the data imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        #  TODO: Incorporate a custom loss function in the training pipeline.\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=Seq2SeqTrainer,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=..,\n",
    "    # callbacks=..\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log experiments\n",
    "with wandb.init(project=\"semantic_parsing_dialog_custom_cost\") as run:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2XeD_gaXGg3"
   },
   "source": [
    "## Computing metrics\n",
    "\n",
    "We strongly recommend you use the given `evaluate_predictions` and `postprocess_text` functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WALCe6Bvac7G"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from semantic_parsing_dialog import evaluate_predictions, postprocess_text\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = postprocess_text(decoded_preds)\n",
    "    decoded_labels = postprocess_text(decoded_labels)\n",
    "    return evaluate_predictions(decoded_labels, decoded_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJg9PMqhXNfv"
   },
   "source": [
    "# Evaluate and compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = '<PATH>'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, tokenizer, batch_size=16):\n",
    "    \"\"\"Evaluates the model on the given dataset\n",
    "    \n",
    "    Both the predictions and the labels should be processed\n",
    "    with `postprocess_text` prior to any metric calculation\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions over test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeVF9yMBXRHt"
   },
   "outputs": [],
   "source": [
    "# make predictions over test set \n",
    "output = trainer.predict(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "katN9vfaXRvj"
   },
   "outputs": [],
   "source": [
    "output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdwGQj7mXR3K"
   },
   "outputs": [],
   "source": [
    "# save predictions\n",
    "predictions = tokenizer.batch_decode(output.predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agenszNUXR9-"
   },
   "outputs": [],
   "source": [
    "with open(\"data/submission_mce.tsv\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(postprocess_text(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AN7q0GBYM3S"
   },
   "source": [
    "# Check in your submission to your branch and make a PR.\n",
    "\n",
    "**please make sure to include your predictions in a file called *data/submission.tsv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N6a2iJuYSpG"
   },
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
